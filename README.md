# Instant Gratification Solution (47th place)

I entered the Instant Gratification competition when it was 16 days to go, so there was already a lot of important information available in Kernels and Discussions. Shortly, in the Instant-Gratification competition we deal with a dataset of the following structure:
    
- [Data seemed to be generated by groups](https://www.kaggle.com/c/instant-gratification/discussion/92930#latest-541375) corresponding to the column **wheezy-copper-turtle-magic**, which means that we have 512 independent datasets of the size approximately 512 by 255 for training our models.

- Out of 255 features [only features with high variance supposed to be important](https://www.kaggle.com/fchmiel/low-variance-features-useless/). The number of important features varies from 33 to 47.

- [QDA proved to be a good modeling approach](https://www.kaggle.com/speedwagon/quadratic-discriminant-analysis) for this case and can give AUC of 0.966. 

- This high quality can be explained by the fact that the [data probably was generated with make_classification function](https://www.kaggle.com/mhviraf/synthetic-data-for-next-instant-gratification).

Knowing that let's look closer at what make_classification function does. This function has the following parameters:

- n_samples: The number of samples.
- n_features: The total number of features.
- n_informative: The number of informative features.
- n_redundant: The number of redundant features, linear combinations of informative features.
- n_repeated: The number of duplicated features.
- n_classes: The number of classes.

- n\_clusters\_per_class: The number of clusters per class.
- weights: The proportions of samples assigned to each class.
- flip_y: The fraction of samples whose class are randomly exchanged.
- class_sep: Larger values spread out the clusters/classes and make the classification task easier.
- hypercube: Parameter corresponding to the placement of the cluster centroids.

- shift: Shift of the feature values.
- scale: Scaling of the features by the specified value.
- shuffle: Shuffling the samples and the features.
- random_state: Random state.

Next, I invite you to follow my sequence of thoughts.

***

### Step 1. Size and features (first 6 parameters of make classification function).

First, from the data structure and size, we can assume that **n_features=255** and **n_samples=1024** were taken to generate both training and test data sets (both private and public).

Number of important features ranges from 33 to 47, so **n_informative** $\in$ **{33,...,47}**. Number of redundant features is **n_redundant=255-n_informative**, while there's no repeated features **n_repeated=0**. Number of classes is obviously **n_classes=2**. From the feature statistics I will assume that parameters shift and scale were set to default **shift=0**, **scale=1**. Parameters **shuffle** and **random_state** are not so important for us right now, because they not change the nature of the data set.

<br>

Second, parameters **n_clusters_per_class, weights, class_sep, hypercube** are the ones that we are not very sure about, especially **n_clusters_per_class**. However, what makes it imposible to get 100% accurate model is that **flip_y>0**, we can not predict this random factor in any case.
